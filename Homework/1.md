# Homework 1 - HPC

1. In following program example we were assuming that each call to `Compute_next_value` requires
   roughly the same amount of work in each loop iteration. For your reference:

   ```cpp
   auto my_sum = 0;
   auto my_first_i = /* ... */;
   auto my_last_i = /* ... */;

   for (auto my_i = my_first_i; my_i < my_last_i; my_i++) {
      auto my_x = Compute_next_value(/* ... */);
      my_sum += my_x;
   }
   ```

   Let's now consider instead that a call when $ i = k $ requires $ k + 1 $ units of time as the call
   when $ i = 0 $. Example: if the first call requires 1 millisecond, the second requires 2ms, the
   third requires 3ms, and so on.

   > Every process get assigned consecutive iterations in order (exactly like in the example in the
   > slides/book), i.e., process 0 will work on the first few iterations, while processor 1 will on
   > the second set of iterations, etc.

   How much time each core will spend in the loop calling the function `Compute_next_value` if
   $ n = 15 $ and $ p = 3 $? Where $n$ is the number of elements in the loop and $p$ in the number of
   available cores.

   **Answer:**

   We'll assume we aren't counting the time to actually distribute the work over the three cores, so
   we'll start by splitting the elements between each core. Since we are assuming that each loop
   iteration takes roughly the same amount of time, we'll just evenly divide the 15 elements between
   the three cores, meaning that we have 3 elements per core.

   15 elements / 3 cores = 5 elements per core

   The time is then trivial to calculate, as we know that the time can be calculated as i + 1 ms, where
   i is the number of elements processed. That means each core will take 6 ms to complete, not accounting
   for setup time.

   | Core | Time (ms) | Possible Elements Processed |
   | :--: | :-------: | :-------------------------: |
   |  0   |    6ms    |             0:4             |
   |  1   |    6ms    |             5:9             |
   |  2   |    6ms    |            10:14            |

2. In the previous question, the load among the cores is not balanced anymore. How would you balance
   it?

   Answer devising an algorithm (written in pseudo-code) which produces the following output:

   - An array containing the total amount of time (i.e., ms) spent on each core
   - A bi-dimensional array containing the indices of the iterations ($i$) you assign to each core
   - Finally, report a numerical example (i.e., including the content of the arrays).

   **Answer:**

   _Will update with compiler explorer link once working, again, not sure what it's asking_

   ```cpp
   // p: number of cores
   template<size_t p>
   auto balance_load(const size_t i) -> pair<array<time>, array<array<indices>>>;
   ```

3. Derive formulas for the number of receives and additions that core 0 and core 4 carry out using

   1. the original pseudo-code for a global sum, and
   2. the tree-structured global sum.

   > Assume $ n $ (number of cores) is always a power of two. Assume the initial partial sums have
   > already been computed by each core.

   Finally, report the numbers of receives and additions carried out by core 0 and core 4 (in two
   different tables) when the two methods are used with 2, 4, 16, 512 and 1024 cores.

   **Answer:**

4. Suppose that your professor is organizing a pizza party for all the students and need your help to
   prepare things faster and clean up afterwards.

   1. Identify tasks for students that will allow them to use "task-parallelism"
   2. How can we use "data-parallelism" to partition the work of cleaning
   3. How could you use a combination of the previous approaches?

   > The use of any type of fruit on the pizza is strongly discouraged!

   **Answer:**

   1. Tasks to be completed:

      - Setting out tables
      - Putting chairs out to go around the tables
      - Setting out plates, forks, and knives
      - Make the pizzas
      - Set the pizzas out for the students to eat
      - Put away tables
      - Throw away trash
      - Refrigerate or freeze pizza
      - Wash off tables

   2. Data parallelism:

      - Put away several chairs at once
      - Throw away multiple trash bags (or pizza boxes) at once

   3. Combination

      - Most tasks has elements that can be parallelized. For example, when setting out tables one
        student could bring several tables to the room at once, then another student could unfold them
        and set them up

5. Write-through Cache

   1. Explain how a write-through cache works.
   2. If we implement in hardware a queue in the CPU, how could this be used to improve the
      performance of a write-through cache?

   **Answer:**

   1. When data is written back into a cache from a register, the updated cache line is written back
      through successive caches back into main memory.

      For example, if you have an integer on the stack,
      `i`, and you perform the assignment `i = i * 3`, the initial value of `i` will be read from the cache
      (thus pulling the cache line containing `i` into the L1 cache) into a register, the register will be
      updated with the new value, `i * 3`, and then that will be written back into the same cache line.
      Once the L1 cache receives the updated value it will proceed to propagate the value into the L2 and
      L3 caches, and finally into main memory

   2. This could act as a buffer that wrote cache lines back into main memory. The benefits of this buffer
      is that other reads and writes could happen in the cache while the write back is occuring.

6. Recall the example involving cache reads of a two-dimensional array. For your reference:

   ```cpp
   /* First pair of loops */

   for (i = 0; i < MAX; i++)
     for (j = 0; j < MAX; j++)
       y[i] += A[i][j]*x[j];

   /* Assign y = 0 */

   /* Second pair of loops */

   for (j = 0; j < MAX; j++)
     for (i = 0; i < MAX; i++)
       y[i] += A[i][j]*x[j];
   ```

   1. How does a larger matrix affect the performance of the two pairs of nested loops?

      **Answer:**

      The first loop will likely take a linearly slower amount of time as the cache reads should be
      relatively good still, where you are missing 1 out of every 4 reads (assuming each cache line
      holds 4 elements and the prefetcher is turned off).

      The second loop also will be slower, probably again linearly, because 4 out of 4 reads are
      missed in the cache.

   2. How does a larger cache affect the performance of the two pairs of nested loops?

      **Answer:**

      For the first loop the only potential performance increase could come from the `x` vector staying 
      in the cache for longer. If on each iteration of `i` we didn't need to refetch `x[0]` into the 
      that would improve our hit/miss ratio. Every other element is only accessed once, so we wouldn't 
      see any other benefit for our `y` accesses or our `A` accesses. 

      For the second loop if the cache was large enough and `A` was small enough to fit in the cache then 
      we would see similar hit/miss ratios to loop 1.

   3. What happens if MAX = 8 and the cache can store four lines? (Hint: draw a table representing the
      cache lines, assume each cache line contains 4 elements of the array)
